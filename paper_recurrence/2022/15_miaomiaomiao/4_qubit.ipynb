{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[WARNING] ME(121778:139723102876480,MainProcess):2023-10-17-23:50:13.444.195 [mindspore/run_check/_check_version.py:102] MindSpore version 2.1.0 and cuda version 11.2.72 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n",
                        "/tmp/ipykernel_121778/2277789754.py:58: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{0}').on([i, i+1])\n",
                        "/tmp/ipykernel_121778/2277789754.py:59: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{0}').on([qubit_num-1, 0])\n",
                        "/tmp/ipykernel_121778/2277789754.py:67: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{2}').on([i, i+1])\n",
                        "/tmp/ipykernel_121778/2277789754.py:68: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{2}').on([qubit_num-1, 0])\n",
                        "/tmp/ipykernel_121778/2277789754.py:134: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  _circ += ZZ('06').on([bit_up, bit_down])\n",
                        "/tmp/ipykernel_121778/2277789754.py:135: DeprecationWarning: YY gate is deprecated, please use Ryy\n",
                        "  _circ += YY('07').on([bit_up, bit_down])\n",
                        "/tmp/ipykernel_121778/2277789754.py:136: DeprecationWarning: XX gate is deprecated, please use Rxx\n",
                        "  _circ += XX('08').on([bit_up, bit_down])\n",
                        "[WARNING] ME(121778:139723102876480,MainProcess):2023-10-17-23:50:14.728.728 [mindspore/train/model.py:1098] For StepAcc callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch: 1 step: 1, loss is 0.6741692423820496\n",
                        "acc is: 0.23809523809523808\n",
                        "epoch: 1 step: 2, loss is 0.7514591217041016\n",
                        "acc is: 0.2857142857142857\n",
                        "epoch: 1 step: 3, loss is 0.7051296234130859\n",
                        "acc is: 0.2857142857142857\n",
                        "epoch: 1 step: 4, loss is 0.7275025248527527\n",
                        "acc is: 0.2857142857142857\n",
                        "epoch: 1 step: 5, loss is 0.7343263030052185\n",
                        "acc is: 0.2857142857142857\n",
                        "epoch: 1 step: 6, loss is 0.7049304842948914\n",
                        "acc is: 0.2857142857142857\n",
                        "epoch: 1 step: 7, loss is 0.6948838233947754\n",
                        "acc is: 0.2857142857142857\n",
                        "epoch: 1 step: 8, loss is 0.6786658763885498\n",
                        "acc is: 0.3333333333333333\n",
                        "epoch: 1 step: 9, loss is 0.7199639678001404\n",
                        "acc is: 0.38095238095238093\n",
                        "epoch: 1 step: 10, loss is 0.7536068558692932\n",
                        "acc is: 0.38095238095238093\n",
                        "epoch: 1 step: 11, loss is 0.6871161460876465\n",
                        "acc is: 0.38095238095238093\n",
                        "epoch: 1 step: 12, loss is 0.693020761013031\n",
                        "acc is: 0.38095238095238093\n",
                        "epoch: 1 step: 13, loss is 0.7279675602912903\n",
                        "acc is: 0.38095238095238093\n",
                        "epoch: 1 step: 14, loss is 0.7251288294792175\n",
                        "acc is: 0.38095238095238093\n",
                        "epoch: 1 step: 15, loss is 0.7115140557289124\n",
                        "acc is: 0.38095238095238093\n",
                        "epoch: 1 step: 16, loss is 0.6861832737922668\n",
                        "acc is: 0.38095238095238093\n",
                        "epoch: 1 step: 17, loss is 0.7235297560691833\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 1 step: 18, loss is 0.7155097126960754\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 1 step: 19, loss is 0.6634852886199951\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 1 step: 20, loss is 0.7179146409034729\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 1, loss is 0.6527130007743835\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 2, loss is 0.7529411911964417\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 3, loss is 0.6910690665245056\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 4, loss is 0.7143685817718506\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 5, loss is 0.7211499214172363\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 6, loss is 0.6899916529655457\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 7, loss is 0.6814541816711426\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 8, loss is 0.6539376378059387\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 9, loss is 0.7095925211906433\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 10, loss is 0.7463987469673157\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 11, loss is 0.6787543296813965\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 12, loss is 0.679502546787262\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 13, loss is 0.7102176547050476\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 14, loss is 0.7163441777229309\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 15, loss is 0.6883767247200012\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 16, loss is 0.671952486038208\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 17, loss is 0.7088229060173035\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 18, loss is 0.6991447806358337\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 19, loss is 0.6434471607208252\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 2 step: 20, loss is 0.7108200192451477\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 1, loss is 0.6371026039123535\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 2, loss is 0.7473134994506836\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 3, loss is 0.6763585209846497\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 4, loss is 0.6975374221801758\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 5, loss is 0.7035408020019531\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 6, loss is 0.6745607256889343\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 7, loss is 0.6685056686401367\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 8, loss is 0.6325425505638123\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 9, loss is 0.6961297988891602\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 10, loss is 0.732203483581543\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 11, loss is 0.6712134480476379\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 12, loss is 0.6662230491638184\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 13, loss is 0.6886820793151855\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 14, loss is 0.7038664817810059\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 15, loss is 0.6635352969169617\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 16, loss is 0.6585778594017029\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 17, loss is 0.6906156539916992\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 18, loss is 0.6801967024803162\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 19, loss is 0.6270318031311035\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 3 step: 20, loss is 0.7006962299346924\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 4 step: 1, loss is 0.6260278820991516\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 4 step: 2, loss is 0.7348580956459045\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 4 step: 3, loss is 0.6617853045463562\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 4 step: 4, loss is 0.6782092452049255\n",
                        "acc is: 0.42857142857142855\n",
                        "epoch: 4 step: 5, loss is 0.6826753616333008\n",
                        "acc is: 0.47619047619047616\n",
                        "epoch: 4 step: 6, loss is 0.6593496203422546\n",
                        "acc is: 0.47619047619047616\n",
                        "epoch: 4 step: 7, loss is 0.6567087173461914\n",
                        "acc is: 0.47619047619047616\n",
                        "epoch: 4 step: 8, loss is 0.6154682636260986\n",
                        "acc is: 0.47619047619047616\n",
                        "epoch: 4 step: 9, loss is 0.6806852221488953\n",
                        "acc is: 0.47619047619047616\n",
                        "epoch: 4 step: 10, loss is 0.711981475353241\n",
                        "acc is: 0.5238095238095238\n",
                        "epoch: 4 step: 11, loss is 0.6650386452674866\n",
                        "acc is: 0.5238095238095238\n",
                        "epoch: 4 step: 12, loss is 0.6542144417762756\n",
                        "acc is: 0.5238095238095238\n",
                        "epoch: 4 step: 13, loss is 0.6651059985160828\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 4 step: 14, loss is 0.6885707974433899\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 4 step: 15, loss is 0.6393172144889832\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 4 step: 16, loss is 0.647411584854126\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 4 step: 17, loss is 0.670480489730835\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 4 step: 18, loss is 0.6604232788085938\n",
                        "acc is: 0.6190476190476191\n",
                        "epoch: 4 step: 19, loss is 0.6164155006408691\n",
                        "acc is: 0.6190476190476191\n",
                        "epoch: 4 step: 20, loss is 0.6883735060691833\n",
                        "acc is: 0.6190476190476191\n",
                        "epoch: 5 step: 1, loss is 0.6212601661682129\n",
                        "acc is: 0.6190476190476191\n",
                        "epoch: 5 step: 2, loss is 0.7162496447563171\n",
                        "acc is: 0.6190476190476191\n",
                        "epoch: 5 step: 3, loss is 0.6492587924003601\n",
                        "acc is: 0.6666666666666666\n",
                        "epoch: 5 step: 4, loss is 0.6583667397499084\n",
                        "acc is: 0.6666666666666666\n",
                        "epoch: 5 step: 5, loss is 0.6607601046562195\n",
                        "acc is: 0.7142857142857143\n",
                        "epoch: 5 step: 6, loss is 0.6463425755500793\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 7, loss is 0.6476690173149109\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 8, loss is 0.6051998138427734\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 9, loss is 0.6650786399841309\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 10, loss is 0.6878350377082825\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 11, loss is 0.6611177325248718\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 12, loss is 0.645074188709259\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 13, loss is 0.6424384117126465\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 14, loss is 0.6724674105644226\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 5 step: 15, loss is 0.6189999580383301\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 5 step: 16, loss is 0.6398822665214539\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 5 step: 17, loss is 0.6511709094047546\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 5 step: 18, loss is 0.6424569487571716\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 5 step: 19, loss is 0.6126285195350647\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 5 step: 20, loss is 0.6758089065551758\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 6 step: 1, loss is 0.6228016018867493\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 2, loss is 0.6948463320732117\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 3, loss is 0.640304684638977\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 4, loss is 0.6407793164253235\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 5, loss is 0.6409881711006165\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 6, loss is 0.6369647979736328\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 7, loss is 0.6421303153038025\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 8, loss is 0.6015818119049072\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 9, loss is 0.6516146063804626\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 10, loss is 0.6642022728919983\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 11, loss is 0.6594602465629578\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 12, loss is 0.6393061280250549\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 13, loss is 0.623648464679718\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 14, loss is 0.658219575881958\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 15, loss is 0.6042325496673584\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 16, loss is 0.6358625292778015\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 17, loss is 0.6353062987327576\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 18, loss is 0.6283358931541443\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 19, loss is 0.6135133504867554\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 20, loss is 0.6651520729064941\n",
                        "acc is: 0.9047619047619048\n",
                        "[0.23809523809523808, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857, 0.3333333333333333, 0.38095238095238093, 0.38095238095238093, 0.38095238095238093, 0.38095238095238093, 0.38095238095238093, 0.38095238095238093, 0.38095238095238093, 0.38095238095238093, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.47619047619047616, 0.47619047619047616, 0.47619047619047616, 0.47619047619047616, 0.47619047619047616, 0.5238095238095238, 0.5238095238095238, 0.5238095238095238, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.6190476190476191, 0.6190476190476191, 0.6190476190476191, 0.6190476190476191, 0.6190476190476191, 0.6666666666666666, 0.6666666666666666, 0.7142857142857143, 0.7619047619047619, 0.7619047619047619, 0.7619047619047619, 0.7619047619047619, 0.7619047619047619, 0.7619047619047619, 0.7619047619047619, 0.7619047619047619, 0.7619047619047619, 0.8095238095238095, 0.8095238095238095, 0.8095238095238095, 0.8095238095238095, 0.8095238095238095, 0.8095238095238095, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048]\n"
                    ]
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# -*- coding: utf-8 -*-\n",
                "\"\"\"\n",
                "Created on Mon Jun 13 17:01:29 2022\n",
                "\n",
                "@author: Waikikilick\n",
                "\"\"\"\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import mindspore as ms\n",
                "from mindquantum import *\n",
                "import mindspore.dataset as ds\n",
                "import matplotlib.pyplot as plt\n",
                "os.environ['OMP_NUM_THREADS'] = '2'\n",
                "import mindspore.context as context\n",
                "from mindspore import Model, load_checkpoint\n",
                "from mindspore.nn import Adam, Accuracy, SoftmaxCrossEntropyWithLogits\n",
                "from mindspore.train.callback import Callback, LossMonitor, ModelCheckpoint, CheckpointConfig\n",
                "\n",
                "ms.context.set_context(mode=ms.context.PYNATIVE_MODE, device_target=\"CPU\") # 训练模式 和 上下文管理器\n",
                "ms.set_seed(1) # 随机数种子\n",
                "np.random.seed(1)\n",
                "\n",
                "class Main():\n",
                "    def __init__(self, batch_size=3): # 定义训练中的批数据大小\n",
                "        super().__init__()\n",
                "        # 导入数据集\n",
                "        self.train_x = np.load('./src/4_train_x.npy', allow_pickle=True)\n",
                "        self.train_y = np.load('./src/4_train_y.npy', allow_pickle=True)\n",
                "        self.eval_x = np.load('./src/4_eval_x.npy', allow_pickle=True)\n",
                "        self.eval_y = np.load('./src/4_eval_y.npy', allow_pickle=True)\n",
                "        self.batch_size = batch_size # 批大小\n",
                "        self.train_dataset = self.build_dataset(self.train_x, self.train_y, self.batch_size) # 构建训练集\n",
                "        self.eval_dataset = self.build_dataset(self.eval_x, self.eval_y, 21) # 构建验证集\n",
                "        self.qnet = MQLayer(self.build_grad_ops()) # 封装网络\n",
                "        self.model = self.build_model() # 建立总模型\n",
                "        self.checkpoint_name = \"./model.ckpt\" # 待保存模型的名字\n",
                "\n",
                "    def build_dataset(self, x, y, batch=None):\n",
                "        train = ds.NumpySlicesDataset(\n",
                "            {  \"image\": x.reshape((x.shape[0], -1)),\n",
                "                \"label\": y.astype(np.int32)\n",
                "            }, shuffle=False) # 因为数据集本来就是随机分布的，所以不用再打乱\n",
                "        if batch is not None:\n",
                "            train = train.batch(batch)\n",
                "        return train\n",
                "\n",
                "    def build_grad_ops(self):\n",
                "        # 构建编码线路\n",
                "        qubit_num = 4\n",
                "        encoder = Circuit()\n",
                "        for i in range(qubit_num):\n",
                "            encoder += H.on(i)\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num-1):\n",
                "            encoder += ZZ(f'{0}').on([i, i+1])\n",
                "        encoder += ZZ(f'{0}').on([qubit_num-1, 0])\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num):\n",
                "            encoder += RX(f'{1}').on(i)\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num-1):\n",
                "            encoder += ZZ(f'{2}').on([i, i+1])\n",
                "        encoder += ZZ(f'{2}').on([qubit_num-1, 0])\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num):\n",
                "            encoder += RX(f'{3}').on(i)\n",
                "        encoder += BarrierGate()\n",
                "        encoder.no_grad() # 编码线路不参与训练\n",
                "\n",
                "        # 构建拟设线路\n",
                "        ansatz = Circuit()\n",
                "        ansatz += conv_circ('00',0,1)\n",
                "        ansatz += conv_circ('01',2,3)\n",
                "        ansatz += pool_circ('02',0,1)\n",
                "        ansatz += pool_circ('03',3,2)\n",
                "\n",
                "        ansatz += conv_circ('10',1,2)\n",
                "        ansatz += pool_circ('11',1,2)\n",
                "\n",
                "        total_circ = encoder.as_encoder() + ansatz.as_ansatz() # 总线路为 编码线路 + 拟设线路\n",
                "\n",
                "        ham = [Hamiltonian(QubitOperator(f'Z{i}')) for i in [1,2]] # 通过测量第 1,2 个比特的测量值来定义分类结果\n",
                "        sim = Simulator('mqvector', total_circ.n_qubits) # 定义模拟器\n",
                "\n",
                "        grad_ops = sim.get_expectation_with_grad( # 梯度和期望值计算接口\n",
                "            ham,\n",
                "            total_circ,\n",
                "            parallel_worker=5)\n",
                "        return grad_ops\n",
                "\n",
                "    def build_model(self):\n",
                "        self.loss = ms.nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean') # 采用交叉熵损失函数\n",
                "        self.opti = ms.nn.Adam(self.qnet.trainable_params()) # 采用 Adam 优化器\n",
                "        self.model = Model(self.qnet, self.loss, self.opti, metrics={'Acc': Accuracy()}) # 装配模型\n",
                "        return self.model\n",
                "\n",
                "    def train(self, epoch=1): # 训练过程中，每一步都计算一次模型对验证集的预测准确度\n",
                "        self.model.train(epoch, self.train_dataset, callbacks=[LossMonitor(1), acc, ckpoint_cb], dataset_sink_mode=False)\n",
                "\n",
                "    def export_trained_parameters(self): # 输出此时的模型\n",
                "        qnet_weight = self.qnet.weight.asnumpy()\n",
                "        ms.save_checkpoint(self.qnet, self.checkpoint_name)\n",
                "\n",
                "    def load_trained_parameters(self): # 导入指定的模型\n",
                "        ms.load_param_into_net(self.qnet,\n",
                "                               ms.load_checkpoint(self.checkpoint_name))\n",
                "\n",
                "    def load_training_ckpoint(self, ckpoint_name): # 导入训练过程中的某个模型\n",
                "        ms.load_param_into_net(self.qnet,\n",
                "                               ms.load_checkpoint(ckpoint_name))\n",
                "\n",
                "    def predict(self, origin_test_x) -> float: # 用于预测\n",
                "        test_x = origin_test_x.reshape((origin_test_x.shape[0], -1))\n",
                "        predict = self.model.predict(ms.Tensor(test_x))\n",
                "        predict = predict.asnumpy().flatten() > 0\n",
                "        return predict\n",
                "\n",
                "\n",
                "def conv_circ(prefix='0', bit_up=0, bit_down=1): # 定义卷积核\n",
                "    _circ = Circuit()\n",
                "    _circ += RX('00').on(bit_up)\n",
                "    _circ += RY('01').on(bit_up)\n",
                "    _circ += RZ('02').on(bit_up)\n",
                "    _circ += RX('03').on(bit_down)\n",
                "    _circ += RY('04').on(bit_down)\n",
                "    _circ += RZ('05').on(bit_down)\n",
                "\n",
                "    _circ += ZZ('06').on([bit_up, bit_down])\n",
                "    _circ += YY('07').on([bit_up, bit_down])\n",
                "    _circ += XX('08').on([bit_up, bit_down])\n",
                "\n",
                "    _circ += RX('09').on(bit_up)\n",
                "    _circ += RY('10').on(bit_up)\n",
                "    _circ += RZ('11').on(bit_up)\n",
                "    _circ += RX('12').on(bit_down)\n",
                "    _circ += RY('13').on(bit_down)\n",
                "    _circ += RZ('14').on(bit_down)\n",
                "    _circ = add_prefix(_circ, prefix)\n",
                "    return _circ\n",
                "\n",
                "def pool_circ(prefix='0', bit_up=0, bit_down=1): # 定义池化操作\n",
                "    _circ = Circuit()\n",
                "    _circ += RX('00').on(bit_up)\n",
                "    _circ += RY('01').on(bit_up)\n",
                "    _circ += RZ('02').on(bit_up)\n",
                "    _circ += RX('03').on(bit_down)\n",
                "    _circ += RY('04').on(bit_down)\n",
                "    _circ += RZ('05').on(bit_down)\n",
                "\n",
                "    _circ += X.on(bit_down, bit_up)\n",
                "\n",
                "    _circ += RX('06').on(bit_down)\n",
                "    _circ += RY('07').on(bit_down)\n",
                "    _circ += RZ('08').on(bit_down)\n",
                "    _circ = add_prefix(_circ, prefix)\n",
                "    return _circ\n",
                "\n",
                "class StepAcc(Callback):  # 定义一个关于每一步准确率的回调函数\n",
                "    def __init__(self, model):\n",
                "        self.model = model\n",
                "        self.acc = []\n",
                "\n",
                "    def step_end(self, run_context):\n",
                "        acc_tem = self.model.eval(main.eval_dataset, dataset_sink_mode=False)['Acc']\n",
                "        print('acc is:', acc_tem) #, 'lr is:', main.opti.get_lr()[0])\n",
                "        self.acc.append(acc_tem)\n",
                "\n",
                "if __name__ == '__main__':\n",
                "\n",
                "    main = Main()\n",
                "    main.ckpoint_file = './ckpoint_file'\n",
                "\n",
                "    acc = StepAcc(main.model)\n",
                "    config_ck = CheckpointConfig(save_checkpoint_steps=1, keep_checkpoint_max=100000000, exception_save=True)\n",
                "    ckpoint_cb = ModelCheckpoint(prefix='9b', directory=main.ckpoint_file, config=config_ck)\n",
                "    sys.stdout.flush()\n",
                "    epoch =  6\n",
                "    lr = 0.01\n",
                "    main.opti = ms.nn.Adam(main.qnet.trainable_params(), lr)\n",
                "    main.train(epoch) # 训练\n",
                "    print(acc.acc) # 输出训练过程中，验证集分类准确度记录\n",
                "\n",
                "    plt.figure()\n",
                "    plt.plot(acc.acc) # 画出训练过程中，验证集分类准确度的变化情况\n",
                "    plt.title('Statistics of accuracy', fontsize=20)\n",
                "    plt.xlabel('Steps', fontsize=20)\n",
                "    plt.ylabel('Accuracy', fontsize=20)\n",
                "    plt.savefig('./src/result_4_qubit.png')\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "<table border=\"1\">\n",
                            "  <tr>\n",
                            "    <th>Software</th>\n",
                            "    <th>Version</th>\n",
                            "  </tr>\n",
                            "<tr><td>mindquantum</td><td>0.9.0</td></tr>\n",
                            "<tr><td>mindspore</td><td>2.1.0</td></tr>\n",
                            "<tr>\n",
                            "    <th>System</th>\n",
                            "    <th>Info</th>\n",
                            "</tr>\n",
                            "<tr><td>Python</td><td>3.9.7</td></tr><tr><td>OS</td><td>Linux x86_64</td></tr><tr><td>Memory</td><td>16.71 GB</td></tr><tr><td>CPU Max Thread</td><td>8</td></tr><tr><td>Date</td><td>Tue Oct 17 23:50:49 2023</td></tr>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<mindquantum.utils.show_info.InfoTable at 0x7f13af77d190>"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from mindquantum.utils.show_info import InfoTable\n",
                "\n",
                "InfoTable('mindquantum', 'mindspore')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        },
        "orig_nbformat": 2
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
