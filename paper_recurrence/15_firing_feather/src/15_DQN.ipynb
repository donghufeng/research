{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from time import *\n",
    "import numpy as np\n",
    "from mindquantum import *\n",
    "from collections import deque\n",
    "from scipy.linalg import expm\n",
    "import matplotlib.pyplot as plt\n",
    "from mindspore import dataset as ds\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import Model, Tensor, nn, set_seed\n",
    "\n",
    "set_seed(1)  # 设置宏观随机数种子\n",
    "\n",
    "sx = X.matrix()\n",
    "sz = Z.matrix()\n",
    "dt = np.pi / 20\n",
    "action_space = [0, 1, 0, 0.5, 1]\n",
    "\n",
    "class env():\n",
    "    def __init__(self,\n",
    "                 action_space=[0, 1, 2],\n",
    "                 dt=0.1):\n",
    "        self.action_space = action_space\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.n_features = 4\n",
    "        self.dt = dt\n",
    "        self.nstep = 0\n",
    "        self.state = np.array([1, 0, 0, 0])\n",
    "        # self.state = np.matrix([[0.+0.j, 0.+0.j], [0.+0.j, 1.+0.j]])\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.array([1, 0, 0, 0]).astype(np.float32)\n",
    "        # self.state = np.matrix([[0.+0.j, 0.+0.j], [0.+0.j, 1.+0.j]])\n",
    "        # state = np.array([init_psi[0].real, init_psi[1].real, init_psi[0].imag, init_psi[1].imag])\n",
    "        self.nstep = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        psi = np.array(\n",
    "            [self.state[0:int(len(self.state) / 2)] + self.state[int(len(self.state) / 2):int(len(self.state))] * 1j])\n",
    "        psi = psi * psi.transpose()\n",
    "        psi = psi.T\n",
    "        psi = np.mat(psi)\n",
    "\n",
    "        J = 4  # control field strength\n",
    "        sx = np.mat([[0, 1], [1, 0]]).astype(np.float32)\n",
    "        sz = np.mat([[1, 0], [0, -1]]).astype(np.float32)\n",
    "\n",
    "        U = np.matrix(np.identity(2))\n",
    "\n",
    "        H = J * float(action) / (self.n_actions - 1) * sz + 1 * sx\n",
    "        U = expm(-1j * H * self.dt)\n",
    "\n",
    "        psi = U * psi  # final state\n",
    "\n",
    "        # state_ = psi\n",
    "        # target = np.mat([[0], [1]], dtype=complex)\n",
    "        target = np.matrix([[1, 0], [0, 0]]).astype(np.float32)\n",
    "\n",
    "        err = 1.0 - np.square(np.abs(psi.H * target)).item(0).real\n",
    "        fid = 1.0 - err\n",
    "\n",
    "        rwd = 10 * (err < 0.5) + 100 * (err < 0.1) + 5000 * (err < 10e-3)\n",
    "\n",
    "        done = ((err < 10e-3) or self.nstep >= np.pi / self.dt)\n",
    "        self.nstep += 1\n",
    "        psi = np.array(psi)\n",
    "        psi_T = psi.T\n",
    "        self.state = np.array(psi_T.real.tolist()[0] + psi_T.imag.tolist()[0])\n",
    "        # self.state = np.array([psi_T[0][0].real, psi_T[1][0].real, psi_T[0][0].imag, psi_T[1][0].imag])\n",
    "\n",
    "        return self.state, rwd, done, fid\n",
    "\n",
    "\n",
    "class LinearNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()  # 网络： 两个隐藏层，分别有 32/32 个神经元；输出层 4 个神经元。\n",
    "        self.fc1 = nn.Dense(4, 64, Normal(0.02), Normal(0.02), True, \"relu\")\n",
    "        self.fc2 = nn.Dense(64, 64, Normal(0.02), Normal(0.02), True, \"relu\")\n",
    "        self.fc3 = nn.Dense(64, 4, Normal(0.02), Normal(0.02), True, \"relu\")\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q_values = self.fc3(x)\n",
    "        return q_values\n",
    "\n",
    "net = LinearNet()\n",
    "\n",
    "class Agent(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 n_actions=4,\n",
    "                 n_features=4,\n",
    "                 learning_rate=0.01,\n",
    "                 reward_decay=0.9,\n",
    "                 e_greedy=0.9,\n",
    "                 replace_target_iter=300,\n",
    "                 memory_size=500,\n",
    "                 batch_size=32,\n",
    "                 e_greedy_increment=None):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if self.epsilon_increment is not None else self.epsilon_max\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        # self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\n",
    "        self.memory_counter = 0\n",
    "        self.main_net = Model(net, nn.loss.MSELoss(),\n",
    "                              nn.RMSProp(params=net.trainable_params(), learning_rate=self.lr))  # 主网络\n",
    "        self.target_net = self.main_net  # 目标网络\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        self.memory.append([s, a, r, s_])\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.argmax(self.main_net.predict(Tensor([state])))\n",
    "        else:\n",
    "            # action = np.random.randint(0, self.n_actions)\n",
    "            # action = random.uniform(-1,1)\n",
    "            action = random.choice([0, 1, 2])\n",
    "        return action\n",
    "\n",
    "    def get_samples(self, s_batch, Q):\n",
    "        for i in range(len(s_batch)):\n",
    "            yield s_batch[i], Q[i]\n",
    "\n",
    "    def get_train_data(self, s_batch, Q):\n",
    "        train_data = ds.GeneratorDataset(list(self.get_samples(s_batch, Q)), column_names=['state', 'Q'])\n",
    "        train_data = train_data.batch(self.batch_size)\n",
    "        return train_data\n",
    "\n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.target_net = self.main_net\n",
    "        if self.memory_counter < self.batch_size:\n",
    "            return\n",
    "        batch_memory = random.sample(self.memory, self.batch_size)\n",
    "        s_batch = np.array([replay[0] for replay in batch_memory]).astype(np.float32)\n",
    "        Q = self.main_net.predict(Tensor(s_batch))\n",
    "        next_s_batch = np.array([replay[3] for replay in batch_memory]).astype(np.float32)\n",
    "        Q_next = self.target_net.predict(Tensor(next_s_batch))\n",
    "\n",
    "        # 更新Q值\n",
    "        for i, replay in enumerate(batch_memory):\n",
    "            _, a, reward, _ = replay\n",
    "            Q[int(i)][int(a)] = reward + self.gamma * max(Q_next[i])\n",
    "        train_data = self.get_train_data(s_batch, Q)\n",
    "        self.main_net.train(1, train_data, dataset_sink_mode=False)\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "\n",
    "# 训练过程\n",
    "def training(ep_max):\n",
    "    training_set = np.array([0, 0, 0, 1])\n",
    "    validation_set = np.array([0, 0, 0, 1])\n",
    "\n",
    "    print('--------------------------')\n",
    "    print('训练中...')\n",
    "    for i in range(ep_max):\n",
    "        if i % 5 == 0:\n",
    "            print('当前训练回合为：', i)\n",
    "        fid_max = 0\n",
    "        observation = env.reset()\n",
    "        nstep = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, fid = env.step(action)\n",
    "            nstep += 1\n",
    "            fid_max = max(fid_max, fid)\n",
    "            agent.store_transition(observation, action, reward, observation_)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if i % 1 == 0:  # 每 x 个回合用验证集验证一下效果\n",
    "            validation_fid_list = []\n",
    "            validation_reward_tot_list = []\n",
    "            for j in range(10):\n",
    "                validation_fid_max = 0\n",
    "                validation_reward_tot = 0\n",
    "\n",
    "                observation = env.reset()\n",
    "                nstep = 0\n",
    "\n",
    "                while True:\n",
    "                    action = agent.choose_action(observation)\n",
    "\n",
    "                    observation_, reward, done, fid = env.step(action)\n",
    "                    nstep += 1\n",
    "                    validation_fid_max = max(validation_fid_max, fid)\n",
    "                    validation_reward_tot = validation_reward_tot + reward * (agent.gamma ** nstep)\n",
    "                    observation = observation_\n",
    "                    if done:\n",
    "                        break\n",
    "                validation_fid_list.append(validation_fid_max)\n",
    "                validation_reward_tot_list.append(validation_reward_tot)\n",
    "            # validation_reward_history.append(np.mean(validation_reward_tot_list))\n",
    "            # validation_fid_history.append(np.mean(validation_fid_list))\n",
    "            if i % 5 == 0:\n",
    "                print('平均保真度: ', np.mean(validation_fid_list))\n",
    "\n",
    "def testing():\n",
    "    ep_max = 50\n",
    "\n",
    "    for i in range(ep_max):\n",
    "\n",
    "        fid_max = 0\n",
    "        observation = env.reset()\n",
    "        nstep = 0\n",
    "        num_actions = []\n",
    "        num_actions_max =[]\n",
    "\n",
    "        while True:\n",
    "            action = agent.choose_action(observation)\n",
    "            num_actions.append(action)\n",
    "            # print(num_actions)\n",
    "            observation_, reward, done, fid = env.step(action)\n",
    "            nstep += 1\n",
    "            if fid > fid_max:\n",
    "                num_actions_max = num_actions\n",
    "            fid_max = max(fid_max, fid)\n",
    "\n",
    "            # sphere = Bloch3d()\n",
    "            # sphere.add_states(states)\n",
    "            # sphere.show()\n",
    "            if done:\n",
    "                break\n",
    "    steps = [i * dt for i in range(len(num_actions_max))]\n",
    "    plt.clf()\n",
    "    plt.step(steps, num_actions_max)\n",
    "    # plt.plot(num_actions, 'ro-', color='#4169E1', alpha=0.8, linewidth=1, label='action')\n",
    "    plt.show()\n",
    "\n",
    "    return fid_max\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = env(action_space=list(range(2)), dt=np.pi / 20\n",
    "              )\n",
    "\n",
    "    agent = Agent(env.n_actions, env.n_features,\n",
    "                  learning_rate=0.01,\n",
    "                  reward_decay=0.9,\n",
    "                  e_greedy=0.99,\n",
    "                  replace_target_iter=200,\n",
    "                  memory_size=2000,\n",
    "                  e_greedy_increment=0.001)\n",
    "\n",
    "    begin_training = time()\n",
    "    training(ep_max=100)\n",
    "    end_training = time()\n",
    "    training_time = end_training - begin_training\n",
    "    print('\\ntraining_time：', training_time, \"s\")\n",
    "    # 测试\n",
    "    testing_fid = testing()\n",
    "    print('testing_fid：', testing_fid)\n",
    "\n",
    "\n"
   ]
  }
 ]
}