import os
os.environ['OMP_NUM_THREADS'] = '2'
from itertools import count
import numpy as np
import time
import copy
import sys
import random
import math
import warnings
import networkx as nx
warnings.filterwarnings("ignore")
from utils import *
from collections import defaultdict

from networkx.algorithms.community.quality import modularity
from networkx.utils import not_implemented_for
from networkx.utils.mapped_queue import MappedQueue
'''
本赛题旨在引领选手探索，在NISQ时代规模有限的量子计算机上，求解真实场景中的大规模图分割问题。
本代码为求解最大割
'''
import resource
soft, hard = resource.getrlimit(resource.RLIMIT_AS)
resource.setrlimit(resource.RLIMIT_AS, (1024*1024*4000, hard)) #4G
N_sub=15 # 量子比特的规模限制

filelist=[ 'graphs/weight_p0.5_n40_cut238.txt',
 'graphs/regular_d3_n80_cut111.txt',
 'graphs/partition_n100_cut367.txt'
]


def _greedy_modularity_communities_generator(G, weight=None, resolution=1):
    
    directed = G.is_directed()
    N = G.number_of_nodes()

    # Count edges (or the sum of edge-weights for weighted graphs)
    m = G.size(weight)
    q0 = 1 / m

    # Calculate degrees (notation from the papers)
    # a : the fraction of (weighted) out-degree for each node
    # b : the fraction of (weighted) in-degree for each node
    if directed:
        a = {node: deg_out * q0 for node, deg_out in G.out_degree(weight=weight)}
        b = {node: deg_in * q0 for node, deg_in in G.in_degree(weight=weight)}
    else:
        a = b = {node: deg * q0 * 0.5 for node, deg in G.degree(weight=weight)}

    # this preliminary step collects the edge weights for each node pair
    # It handles multigraph and digraph and works fine for graph.
    dq_dict = defaultdict(lambda: defaultdict(float))
    for u, v, wt in G.edges(data=weight, default=1):
        if u == v:
            continue
        dq_dict[u][v] += wt
        dq_dict[v][u] += wt

    # now scale and subtract the expected edge-weights term
    for u, nbrdict in dq_dict.items():
        for v, wt in nbrdict.items():
            dq_dict[u][v] = q0 * wt - resolution * (a[u] * b[v] + b[u] * a[v])

    # Use -dq to get a max_heap instead of a min_heap
    # dq_heap holds a heap for each node's neighbors
    dq_heap = {u: MappedQueue({(u, v): -dq for v, dq in dq_dict[u].items()}) for u in G}
    # H -> all_dq_heap holds a heap with the best items for each node
    H = MappedQueue([dq_heap[n].heap[0] for n in G if len(dq_heap[n]) > 0])

    # Initialize single-node communities
    communities = {n: frozenset([n]) for n in G}
    yield communities.values()

    # Merge the two communities that lead to the largest modularity
    while len(H) > 1:
        # Find best merge
        # Remove from heap of row maxes
        # Ties will be broken by choosing the pair with lowest min community id
        try:
            negdq, u, v = H.pop()
        except IndexError:
            break
        dq = -negdq
        yield dq
        # Remove best merge from row u heap
        dq_heap[u].pop()
        # Push new row max onto H
        if len(dq_heap[u]) > 0:
            H.push(dq_heap[u].heap[0])
        # If this element was also at the root of row v, we need to remove the
        # duplicate entry from H
        if dq_heap[v].heap[0] == (v, u):
            H.remove((v, u))
            # Remove best merge from row v heap
            dq_heap[v].remove((v, u))
            # Push new row max onto H
            if len(dq_heap[v]) > 0:
                H.push(dq_heap[v].heap[0])
        else:
            # Duplicate wasn't in H, just remove from row v heap
            dq_heap[v].remove((v, u))

        # Perform merge
        communities[v] = frozenset(communities[u] | communities[v])
        del communities[u]

        # Get neighbor communities connected to the merged communities
        u_nbrs = set(dq_dict[u])
        v_nbrs = set(dq_dict[v])
        all_nbrs = (u_nbrs | v_nbrs) - {u, v}
        both_nbrs = u_nbrs & v_nbrs
        # Update dq for merge of u into v
        for w in all_nbrs:
            # Calculate new dq value
            if w in both_nbrs:
                dq_vw = dq_dict[v][w] + dq_dict[u][w]
            elif w in v_nbrs:
                dq_vw = dq_dict[v][w] - resolution * (a[u] * b[w] + a[w] * b[u])
            else:  # w in u_nbrs
                dq_vw = dq_dict[u][w] - resolution * (a[v] * b[w] + a[w] * b[v])
            # Update rows v and w
            for row, col in [(v, w), (w, v)]:
                dq_heap_row = dq_heap[row]
                # Update dict for v,w only (u is removed below)
                dq_dict[row][col] = dq_vw
                # Save old max of per-row heap
                if len(dq_heap_row) > 0:
                    d_oldmax = dq_heap_row.heap[0]
                else:
                    d_oldmax = None
                # Add/update heaps
                d = (row, col)
                d_negdq = -dq_vw
                # Save old value for finding heap index
                if w in v_nbrs:
                    # Update existing element in per-row heap
                    dq_heap_row.update(d, d, priority=d_negdq)
                else:
                    # We're creating a new nonzero element, add to heap
                    dq_heap_row.push(d, priority=d_negdq)
                # Update heap of row maxes if necessary
                if d_oldmax is None:
                    # No entries previously in this row, push new max
                    H.push(d, priority=d_negdq)
                else:
                    # We've updated an entry in this row, has the max changed?
                    row_max = dq_heap_row.heap[0]
                    if d_oldmax != row_max or d_oldmax.priority != row_max.priority:
                        H.update(d_oldmax, row_max)

        # Remove row/col u from dq_dict matrix
        for w in dq_dict[u]:
            # Remove from dict
            dq_old = dq_dict[w][u]
            del dq_dict[w][u]
            # Remove from heaps if we haven't already
            if w != v:
                # Remove both row and column
                for row, col in [(w, u), (u, w)]:
                    dq_heap_row = dq_heap[row]
                    # Check if replaced dq is row max
                    d_old = (row, col)
                    if dq_heap_row.heap[0] == d_old:
                        # Update per-row heap and heap of row maxes
                        dq_heap_row.remove(d_old)
                        H.remove(d_old)
                        # Update row max
                        if len(dq_heap_row) > 0:
                            H.push(dq_heap_row.heap[0])
                    else:
                        # Only update per-row heap
                        dq_heap_row.remove(d_old)

        del dq_dict[u]
        # Mark row u as deleted, but keep placeholder
        dq_heap[u] = MappedQueue()
        # Merge u into v and update a
        a[v] += a[u]
        a[u] = 0
        if directed:
            b[v] += b[u]
            b[u] = 0

        yield communities.values()

def greedy_modularity_communities(
    G,
    weight=None,
    resolution=1,
    cutoff=1,
    best_n=None,
):
    
    if (cutoff < 1) or (cutoff > G.number_of_nodes()):
        raise ValueError(f"cutoff must be between 1 and {len(G)}. Got {cutoff}.")
    if best_n is not None:
        if (best_n < 1) or (best_n > G.number_of_nodes()):
            raise ValueError(f"best_n must be between 1 and {len(G)}. Got {best_n}.")
        if best_n < cutoff:
            raise ValueError(f"Must have best_n >= cutoff. Got {best_n} < {cutoff}")
        if best_n == 1:
            return [set(G)]
    else:
        best_n = G.number_of_nodes()

    # retrieve generator object to construct output
    community_gen = _greedy_modularity_communities_generator(
        G, weight=weight, resolution=resolution
    )

    # construct the first best community
    communities = next(community_gen)

    # continue merging communities until one of the breaking criteria is satisfied
    while len(communities) > cutoff:
        try:
            dq = next(community_gen)
        # StopIteration occurs when communities are the connected components
        except StopIteration:
            communities = sorted(communities, key=len, reverse=True)
            # if best_n requires more merging, merge big sets for highest modularity
            while len(communities) > best_n:
                comm1, comm2, *rest = communities
                communities = [comm1 ^ comm2]
                communities.extend(rest)
            return communities

        # keep going unless max_mod is reached or best_n says to merge more
        if dq < 0 and len(communities) <= best_n:
            break
        communities = next(community_gen)

    return sorted(communities, key=len, reverse=True)



def graph_partition(g,G,N_sub) -> list:
    
    H = []
    v = list(g.nodes)
 
    n_g = math.ceil(len(v) / N_sub)
    np.random.shuffle(v)
    sub_list = [v[N_sub*i:N_sub*(i+1)] for i in range(n_g)]
    #print(nx.community.modularity(g, sub_list))
    return sub_list


def calc_subqubo_m(sub_index, x, J, h=None,C=0.0 ):
    x = np.sign(x-0.5)
    
    J_sub=(J[:,sub_index])[sub_index]  
    h_sub=np.array([0]*len(sub_index))
    C_sub=float(C)
    
    return J_sub,h_sub,C_sub



def build_sub_qubo1(g,solution,N_sub,J,h=None,C=0.0):
    sub_list=[]
    #sub_list=graph_partition(g,J,N_sub)
    cuts=greedy_modularity_communities(g,weight=None,resolution=8,cutoff=9,best_n=9)
    for i in range(len(cuts)):
        cut=list(cuts[i])
        sub_list.append(cut)
        #print(len(cut))
    #print(nx.community.modularity(g, cuts))
    J_sub=[]
    h_sub=[]
    C_sub=[]
    #print('subindex:',sub_index)
    for sub_index in sub_list:
        J_sub_temp,h_sub_temp,C_sub_temp = calc_subqubo_m(sub_index, solution, J, h=h,C=C )
        J_sub.append(J_sub_temp)
        h_sub.append(h_sub_temp)
        C_sub.append(C_sub_temp)
    return sub_list,J_sub,h_sub,C_sub


def calc_qubo2(sol,G,sub_list,h=None,C=0.0):
    adjoint=csr_matrix((len(sub_list), len(sub_list)))
    for i in range(len(sub_list)):
        for j in range(i+1,len(sub_list)):
            w_pos=0
            w_neg=0
            for x in range(len(sub_list[i])):
                for y in range(len(sub_list[j])):
                    m,n=sub_list[i][x],sub_list[j][y]
                    w_pos+=(sol[i][x]!=sol[j][y])*G[m,n]
                    w_neg+=(sol[i][x]==sol[j][y])*G[m,n]
            adjoint[i,j]=w_neg-w_pos
            adjoint[j,i]=w_neg-w_pos
   
    h=np.array([0]*len(sub_list))
    C=float(C)
    return adjoint, h, C



def solve_equation_system(F, y):
    '''
    使用python求解多元一次方程组，矩阵形式为Fx = y, F为n*m矩阵，
    x,y为m*1矩阵。要求：当问题有多个解时，返回其中一个解。当问题有唯一解时，返回唯一解。当问题无解时，用线性回归分析，求出一个误差比较少的解。
    '''
    try:
        # 求解方程组
        x = np.linalg.solve(F, y)
        return x
    except np.linalg.LinAlgError:
        # 如果方程组没有解或有多个解，使用线性回归分析求解
        x = np.linalg.lstsq(F, y, rcond=None)[0]
        return x
    


def costFun(J,h,x):
    cost = x.T@J@x + h.T@x
    cost = cost/2 - 0.5* J.sum()
    return cost

def get_index(g, node): #node在原图中是第几个点，在迭代过程中，A中点不断删除，因此节点i并非第i个节点，需保证相对顺序与旧图一致
    return list(g.nodes).index(node)

    
class GraphDecompositionSolver():
    def __init__(self, J, g = None, h = None, c = 0):
        if not isinstance(J, np.ndarray):
            self.J = J.toarray() 
        else:
            self.J = J
        self.qubits = len(self.J)
        if h is None:
            h = np.zeros(self.qubits)
        if g is None:
            self.g = nx.Graph()
            self.g.add_node(0)
            for i in range(self.qubits):
                for j in range(i+1, self.qubits):
                    self.g.add_node(j)
                    if np.abs(self.J[i][j]) > 1e-4:
                        self.g.add_edge(i, j)
        else:
            self.g = g
        if h is None:
            h = np.zeros(self.qubits)
        self.h = h
        self.c = c
        self.end_qubits = 5

        self.iter_process = {}
        
    def solve(self):  
        g =  self.g.copy()
        J =  self.J.copy()
        h = self.h.copy()
        c = self.c                    
        simplify = max(dict(g.degree).values()) >= 10             
        while len(g.nodes) > self.end_qubits:
            #print(len(g.nodes))
            gdis = GraphDecompositionIterationSolver(g, J, h, c, simplify)
            g = gdis.new_g
            J = gdis.new_J
            h = gdis.new_h
            c = gdis.new_c
            self.iter_process[len(g.nodes)] = gdis
        sol = 2*QAOAsolver(csr_matrix(J), h, c, depth = 2, tol=1e-4, info=False)-1
        for i in range(len(g.nodes), self.qubits):
            #print(i)
            sol = self.iter_process[i].recover(sol)
        return sol
            

        
    
    
    
class GraphDecompositionIterationSolver():
    '''
    给定一个旧图，需得到一个节点更少新图，使得新图的maxcut和旧图一致。
    '''
    def __init__(self, old_g, old_J, old_h, old_c = 0, simplified = True):
        '''
        old_g: 图，主要描述节点顺序，与二次项，一次项系数对应
        old_J: 二次项系数
        old_h: 一次项系数
        old_c: 常数 
        '''
        self.simplified = simplified #复杂图需要简化
        self.old_g = old_g
        self.old_J = old_J 
        self.old_h = old_h
        self.old_c = old_c
        self.Ax = {} #为了记录B确定时，A节点的结果。由于A点只有1个点，直接比较即可
        self.A, self.B, self.C = self.parationed() #将原图分成三块
        self.A_old_index =  [get_index(self.old_g, node) for node in self.A]
        self.B_old_index =  [get_index(self.old_g, node) for node in self.B]
        self.C_old_index =  [get_index(self.old_g, node) for node in self.C]
        self.A_old_index.sort()
        self.B_old_index.sort()
        self.C_old_index.sort()
        #print("B:", len(self.B))
        self.k = len(self.B)
        self.n = len(self.old_J)
        self.new_g = self.old_g.subgraph(self.B | self.C).copy()
        self.B_new_index =  [get_index(self.new_g, node) for node in self.B]
        self.C_new_index =  [get_index(self.new_g, node) for node in self.C]
        self.B_new_index.sort()
        self.C_new_index.sort()
        
        self.new_J = np.zeros((self.n-1, self.n-1)) 
        self.new_h = np.zeros(self.n-1) 
        self.new_c = 0
        self.transfer_coef()
    
        
    def parationed(self):
        '''
        对图进行分块A,B,C。A中是一个邻点数最少的节点， B中是他的邻点，C中是其他节点。A,B,C存储的是点的序号
        ''' 
        dict_degree=  dict(self.old_g.degree)
        #由于题目对精度的重要性大于速度，A的数量取1.若兼顾速度，可酌情增大，但是calculate_cmaxs需要重构。
        A = set([min(dict_degree, key=lambda x: dict_degree[x])])
        B = set(self.old_g.neighbors(list(A)[0]))
        C = set(self.old_g.nodes)-A-B
        return A, B, C
    
            
    def calculate_c(self, s):
        '''
        计算B内常数项, g是子图， J是子图下的二次项(0,1), h是子图下的一次项(0,1)
        '''    

        mask1 = np.ix_(self.B_old_index, self.B_old_index)
        mask2 = np.ix_(self.B_old_index)
        return s.T @ self.old_J[mask1] @s + s.T @ self.old_h[mask2] 
    
    def  calculate_cmaxs(self, s):
        '''
        给定s，计算变量为A的最大cost.A只有一个节点，取+1/-1时符号相反，因此只需要将结果取绝对值。
        '''

        mask = np.ix_(self.A_old_index, self.B_old_index)
        cmaxs = 2*s.T @ self.old_J[mask].reshape(-1) + self.old_h[self.A_old_index[0]]
        if cmaxs > 0:
            self.Ax[tuple(s)] = 1
        else:
            self.Ax[tuple(s)] = -1
        return np.abs(cmaxs)

      
        
    def transfer_coef(self):
        '''
        以函数0，1下
        '''

        y = []
        F_list = [] #先是二次项，再是一次项，然后常数项
        s_list = self.get_sList()
        mask = np.ix_(self.B_old_index, self.B_old_index)
        for binary in s_list: #将时间复杂度从2**n,下降为2**k
            F = (self.k*(self.k-1)//2+self.k+1)*[0]
            cmaxs = self.calculate_cmaxs(binary)
            cmaxs += self.calculate_c(binary) 
            y.append(cmaxs)
            
            #F的顺序是二次项,一次项和常数项
            t = 0
            for j in range(self.k):
                for k in range(j+1, self.k):
                    if np.abs(self.old_J[mask][j][k])<1e-4 and self.simplified:
                        continue
                    F[t] = binary[j] *  binary[k]
                    t += 1
            for j in range(self.k):
                F[t] = binary[j]
                t += 1 
            F[t] = 1
            F_list.append(F)
        #方程等式的目的就是让新图中B内的cost恰好等于旧图中A+B的cost。   
        # x的顺序是二次项,一次项和常数项, 即为新图中J，h,c权重。
        # print('F_list', F_list)
        # print('y', y)
        x =  solve_equation_system(np.array(F_list), np.array(y))
        #print(x)
        self.generate_new_graph(x)
        
    def generate_new_graph(self, x):
        bc_index = self.B_old_index+self.C_old_index
        bc_index.sort()
        mask1 = np.ix_(bc_index,bc_index)
        mask2 = np.ix_(bc_index)
        self.new_J = self.old_J[mask1].copy()
        self.new_h = self.old_h[mask2].copy()
        t = 0
        for i in self.B_new_index:
            for j in self.B_new_index:
                if j <=i:
                    continue
                node_i = list(self.new_g.nodes)[i]
                node_j = list(self.new_g.nodes)[j]
                if self.simplified:
                    if self.new_g.has_edge(node_i, node_j):
                        self.new_J[i,j] = 0.5 * float(x[t])
                        self.new_J[j,i] = self.new_J[i,j]
                        t += 1
                else:
                    self.new_J[i,j] = 0.5 * float(x[t])
                    self.new_J[j,i] = self.new_J[i,j]
                    t += 1
                    if not self.new_g.has_edge(node_i, node_j):
                        self.new_g.add_edge(node_i, node_j)
        for i in self.B_new_index:
            node_i = list(self.new_g.nodes)[i]
            self.new_h[i] = float(x[t])
            t += 1
        self.new_c = float(x[t]) + self.old_c
        #print(len(self.new_g.edges))

        
    def recover(self, sol):
        '''
        反迭代过程，从子图中已知节后，输出A的解
        '''
        
        B_value = [sol[i] for i in self.B_new_index]
        A_value = self.get_optimal_A(B_value)
        sol = list(sol)
        sol.insert(self.A_old_index[0], A_value)
        return np.array(sol)
        
    def get_sList(self, n=12):
        #如果B中的点大于等于15，运算很慢，以此s不能遍历到所有结果
        sList = [] 
        k = min(self.k, n)
        if self.k > n:
            mask = np.ix_(self.B_old_index, self.B_old_index)
            h= np.abs(np.sum(self.old_J[mask],0)) #B中一次项系数，相对顺序未变
            sorted_lst = sorted(enumerate(h), key=lambda x: x[1], reverse=False)
            top_n_indices = [t[0] for t in sorted_lst[:self.k-n]] #最大的几个数在h中的index
            top_n_indices.sort()
        for i in range(2**k): #将时间复杂度从2**n,下降为2**k
            binary = bin(i)[2:]
            binary = (k-len(binary))*'0'+binary
            binary = np.array([2*int(j)-1 for j in binary])
            if self.k > n:
                for j in top_n_indices:
                    binary = np.insert(binary, j, [2*np.random.randint(2)-1])
            sList.append(binary)
        return sList
    
    def get_optimal_A(self, B_value):
        if tuple(B_value) in self.Ax:
            return self.Ax[tuple(B_value)]
        else:
            sign = 2*(self.calculate_cmaxs(np.array(B_value))>0)-1
            return sign
        


def solve1(solution,G):
    '''
    自定义求解函数。
    例如可简单通过不断抽取N_sub个变量组成subqubo问题并对子问题进行求解，最终收敛到一个固定值。
    或者可采取其他方法...
    
    【注】可任意改变求解过程，但不可使用经典算法如模拟退火，禁忌搜索等提升解质量。请保持输入输出一致。
    
    输入：
    sol （numpy.array）：初始随机0/1比特串，从左到右分别对应从第1到第N个问题变量。
    G （matrix): QUBO问题矩阵
    
    输出：
    sol （numpy.array）：求解结果的0/1比特串
    cut_temp （float）：最终结果的cut值
    '''
    g_test = from_scipy_sparse_array(-2*G)
    degree = max(dict(g_test.degree).values())

    if degree<20:
        gds = GraphDecompositionSolver(G)
        solution = gds.solve()
        cut = calc_cut_x(G, solution)
    else:
        sub_list,J_sub,h_sub,C_sub=build_sub_qubo1(g_test,solution,N_sub,G,h=None,C=0)
        sol=[]
        cut_initial =calc_cut_x(G, solution)
        
        for i in range(len(sub_list)):
        #print(i)
        #print('before sol:',calc_qubo_x(G, sol))
            solution=solve_QAOA(J_sub[i],h_sub[i],C_sub[i],sub_list[i],solution,depth=3,tol=1e-5)# You can change the depth and tolerance of QAOA solver
            sol.append(solution[sub_list[i]])
        
        adjoint, h, C=calc_qubo2(sol,G,sub_list,h=None,C=0.0)
        sol_flip=QAOAsolver(adjoint,h,C,depth=3,tol=1e-5)
   
        for i in range(len(sol_flip)):
            if sol_flip[i]==0:
                #print('flip before')
                #print(sol[i])
                sol[i]=1-sol[i]
                #print('flip')
                #print(sol[i])
                solution[sub_list[i]]=sol[i]

        cut=calc_cut_x(G, solution)
    #print('sol', sol)
    return solution, cut

def from_scipy_sparse_array(
    A, parallel_edges=False, create_using=None, edge_attribute="weight"
):
    
    G = nx.empty_graph(0, create_using)
    n, m = A.shape
    if n != m:
        raise nx.NetworkXError(f"Adjacency matrix not square: nx,ny={A.shape}")
    # Make sure we get even the isolated nodes of the graph.
    G.add_nodes_from(range(n))
    # Create an iterable over (u, v, w) triples and for each triple, add an
    # edge from u to v with weight w.
    triples = _generate_weighted_edges(A)
   
    if A.dtype.kind in ("i", "u") and G.is_multigraph() and parallel_edges:
        chain = itertools.chain.from_iterable
       
        triples = chain(((u, v, 1) for d in range(w)) for (u, v, w) in triples)
    
    if G.is_multigraph() and not G.is_directed():
        triples = ((u, v, d) for u, v, d in triples if u <= v)
    G.add_weighted_edges_from(triples, weight=edge_attribute)
    return G

def _generate_weighted_edges(A):
    """Returns an iterable over (u, v, w) triples, where u and v are adjacent
    vertices and w is the weight of the edge joining u and v.

    `A` is a SciPy sparse array (in any format).

    """
    if A.format == "csr":
        return _csr_gen_triples(A)
    if A.format == "csc":
        return _csc_gen_triples(A)
    if A.format == "dok":
        return _dok_gen_triples(A)
    # If A is in any other format (including COO), convert it to COO format.
    return _coo_gen_triples(A.tocoo())

def _csr_gen_triples(A):
    """Converts a SciPy sparse array in **Compressed Sparse Row** format to
    an iterable of weighted edge triples.

    """
    nrows = A.shape[0]
    data, indices, indptr = A.data, A.indices, A.indptr
    for i in range(nrows):
        for j in range(indptr[i], indptr[i + 1]):
            yield i, indices[j], data[j]

   
def build_sub_qubo2(solution,N_sub,J,h=None,C=0.0):
   
    delta_L=[]
    for j in range(len(solution)):
        copy_x=copy.deepcopy(solution)
        copy_x[j]=1-copy_x[j]
        x_flip=copy_x
        sol_qubo=calc_qubo_x(J,solution,h=h,C=C)
        x_flip_qubo=calc_qubo_x(J,x_flip,h=h,C=C)
        delta=x_flip_qubo-sol_qubo
        delta_L.append(delta)
    delta_L=np.array(delta_L)
    #print(delta_L)
    sub_index = np.argpartition(delta_L, -N_sub)[-N_sub:] # subqubo子问题的变量们
    #print('subindex:',sub_index)
    J_sub,h_sub,C_sub = calc_subqubo(sub_index, solution, J, h=h,C=C )
    return sub_index,J_sub,h_sub,C_sub

def solve(sol,G):

    i=0
    cut_list=[]
    while(i<4):
        print(f'---{i}---')
        sub_index,J_sub,h_sub,C_sub=build_sub_qubo2(sol,N_sub,G,h=None,C=0)
        cut_before=calc_cut_x(G, sol)
        cut_list.append(cut_before)
        print('before sol:',cut_before)
        sol=solve_QAOA(J_sub,h_sub,C_sub,sub_index,sol,depth=3,tol=1e-5) # You can change the depth and tolerance of QAOA solver
        qubo_temp=calc_qubo_x(G, sol)
        cut_temp =calc_cut_x(G, sol)
        print('after subqubo:',qubo_temp,'|cut:',cut_temp)
        i+=1
        cut_list.append(cut_temp)
        '''
        if cut_before>cut_temp:
            cut_return=cut_before
        else:
            cut_return=cut_temp
        '''
        cut_return=np.max(cut_list)
        print('cut_return',cut_return)
    return cut_return       



def run():
    """
    Main run function, for each graph need to run for 20 times to get the mean result.
    Please do not change this function, we use this function to score your algorithm. 
    """
    cut_list=[]
    cut_f_list=[]
    for filename in filelist[:]:
        print(f'--------- File: {filename}--------')
        g,G=read_graph(filename)
        n=len(g.nodes) # 图整体规模
        cuts=[]
        cuts_f=[]
        for turn in range(20):
            print(f'------turn {turn}------')
            sol=init_solution(n) # 随机初始化解   
            qubo_start = calc_qubo_x(G, sol)
            cut_start =calc_cut_x(G, sol)
            #print('origin qubo:',qubo_start,'|cut:',cut_start)
            solution,cut=solve1(sol, G) #主要求解函数, 主要代码实现
            cut_f=solve(solution, G)
            cuts.append(cut)
            cuts_f.append(cut_f)
        cut_list.append(cuts)
        cut_f_list.append(cuts_f)    
    return np.array(cut_list),  np.array(cut_f_list)
        



if __name__== "__main__": 
    #计算分数

    start=time.time()
    cut_list,cut_f_list=run()
    end=time.time()
    print(cut_list,cut_f_list)
    score=np.array(np.mean(cut_list,axis=1))
    score_f=np.array(np.mean(cut_f_list,axis=1))
    #print('mean cut',score)
    print('score:',np.sum(score))
    print('score_f:',np.sum(score_f))
    #print('time:', (end-start)/60,'min')
     
            



  
    
 
    
            